Purpose
- Build Phoenix-RLM Investigator: an RLM-powered investigation layer on top of Arize Phoenix.
- First milestone: run a Phoenix tutorial agent locally (no Docker) to generate traces, then run evals on those traces.
- Core evaluator capabilities after trace setup:
  - Trace RCA
  - Policy-to-Trace Compliance Auditor (evidence-based approvals)
  - Incident dossier generation

Naming
- Project name: Phoenix-RLM Investigator (use consistently).
- Do not introduce reserved internal codenames into repo text, code, or schemas.

Status
- Phases 1–5 are actively in scope.
- RLM is the core implementation path for Trace RCA, Policy-to-Trace Compliance, and incident dossier generation.

RLM engine topology (locked)
- We run three separate RLM engines, one per capability:
  - Trace RCA engine
  - Policy-to-Trace Compliance engine
  - Incident dossier engine
- All three engines must implement the same shared runtime contract:
  - `specs/rlm_runtime_contract.md`
- Shared runtime contract defines sandbox limits, recursion budgets, and run artifact requirements.
- Engine-specific differences (prompts, narrowing logic, selection logic) are allowed only behind the shared contract.

Core invariants (must not break)
- Evidence comes only from Phoenix trace/span data and stored artifacts. Never claim access to hidden model reasoning.
- All investigator outputs must be:
  1) structured (JSON),
  2) evidence-linked (trace_id/span_id/artifact_id),
  3) reproducible (rerunnable on the same trace dataset).
- Every RCA/compliance/dossier evaluator run must emit a RunRecord-equivalent artifact, including failed or partial runs.
- Investigation runs asynchronously/offline (not on the critical path of the agent response).
- If something is confusing or ambiguous, update these docs so it does not repeat.

Repo reality
- `phoenix/` and `rlm/` are reference repos (standalone git repos). Treat them as read-only unless Nainy asks otherwise.
- This repo will host:
  - `apps/` runnable demo agent(s) + failure injectors
  - `investigator/` three evaluator engines (RCA/compliance/incident) + shared runtime + schemas + inspection API
  - `connectors/` optional logs/metrics connectors (later)
- Legacy notes/specs from an unrelated prior project live under `archive/plan1_specs/` and are not part of Phoenix-RLM Investigator.

Phase 1 canonical loop (no Docker)
1) Start Phoenix (daemon):
   - `python -m pip install arize-phoenix`
   - `phoenix serve`
   - UI: `http://localhost:6006`
2) Configure tracing for local Phoenix:
   - Prefer setting `PHOENIX_COLLECTOR_ENDPOINT=http://127.0.0.1:6006` (base URL) and calling `phoenix.otel.register(protocol="http/protobuf")`.
   - If a tutorial requires an explicit OTLP HTTP endpoint, use `http://127.0.0.1:6006/v1/traces`.
3) Run the tutorial agent end-to-end and confirm traces appear:
   - First choice: `phoenix/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb`
   - Fallback: `phoenix/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb`
4) Generate a seeded failure dataset (30–100 traces) with an external ground-truth manifest:
   - Manifest maps `run_id` and/or `trace_id` to `expected_label` (do not write expected labels into trace metadata).
   - Store manifest under `datasets/seeded_failures/manifest.json` (or `.yaml`).
   - Store exported trace data as Parquet under `datasets/seeded_failures/exports/` (gitignored by default).

OpenAI usage (Phase 1-4)
- Allowed: OpenAI API.
- Defaults:
  - primary model: `gpt-4o-mini` (all calls — root and sub-calls)
  - optional upgrade for root synthesis: `gpt-4o` or `gpt-5.2`
- Budget discipline:
  - default iteration cap: ~$5/day while harness is unstable; increase only after eval loop is reliable.

Project phases (the order matters)

Phase 1 — Traces first
Goal: run a real example agent (from Phoenix tutorials) and generate traces in Phoenix.
Definition of done:
- Phoenix running locally
- agent runnable locally
- traces visible + navigable in Phoenix
- seeded failures (30–100 traces) recorded for RCA + incident investigations

Phase 2 — Trace RCA (use case 2)
Goal: RLM evaluator that labels traces with failure mode + evidence pointers + remediation.
Definition of done:
- RCA results written back into Phoenix as evals/annotations
- RCA taxonomy stable; majority of seeded failures labeled correctly
- each RCA cites evidence `span_id` and (when relevant) tool/retrieval artifact IDs
- RCA is generated by recursive Inspection API calls (not a single monolithic prompt)

Phase 3 — Policy-to-Trace Compliance Auditor
Goal: RLM auditor that evaluates trace behavior against internal AI controls and emits evidence-based compliance decisions.
Definition of done:
- compliance findings written back into Phoenix as evals/annotations
- each finding cites control_id + pass/fail + severity + confidence + evidence pointers
- governance reviewer can click findings to exact spans/artifacts in Phoenix
- auditor is generated through recursive, evidence-driven API exploration

Phase 4 — Incident investigation (use case 3)
Goal: RLM incident dossier generator correlating traces + deploy/config context (logs/metrics later).
Definition of done:
- given an incident trigger (time window + query), produces dossier JSON
- dossier includes a timeline, ranked hypotheses, actions, and evidence pointers
- dossier is written back to Phoenix (trace-linked)
- dossier is generated through recursive, evidence-driven API exploration

Phase 5 — RLM hardening and expansion
Goal: harden and scale the existing RLM evaluator runtime, not replace a baseline.
Definition of done:
- RLM evaluator uses a read-only Inspection API (trace/log/metric/config)
- outputs match the same RCA/compliance/dossier schemas (drop-in replacement)
- sandbox restrictions enforced (no network/filesystem access from the REPL; quotas)
- recursion budgets, tool-call quotas, and failure handling are validated by tests

Canonical commands (keep these current)
- Environment (uv-first):
  - Install `uv` (one-time, system): see https://docs.astral.sh/uv/ (keep this section short; prefer linking)
  - Create/sync env (once we add `pyproject.toml`): `uv sync`
  - Run Python: `uv run python -m <module>`
- Start Phoenix (no Docker):
  - `python -m pip install arize-phoenix`
  - `phoenix serve`  # UI: http://localhost:6006
- Local Phoenix endpoint (base URL):
  - `PHOENIX_COLLECTOR_ENDPOINT=http://127.0.0.1:6006`
- Recommended first tutorial agent:
  - `phoenix/tutorials/tracing/langgraph_agent_tracing_tutorial.ipynb`
  - fallback: `phoenix/tutorials/tracing/llama_index_openai_agent_tracing_tutorial.ipynb`

Where specs live
- Spec pin index: `specs/README.md`
- Trace Inspection API: `API.md`
- System architecture + dataflow: `ARCHITECTURE.md`
- Evaluator design, schemas, and write-back strategy: `DESIGN.md`

Run artifact requirement (all phases)
- Required output for every evaluator invocation:
  - `artifacts/investigator_runs/<run_id>/run_record.json`
- Minimum required fields:
  - `run_id`, `run_type` (`rca` or `policy_compliance` or `incident_dossier`), `status`, `started_at`, `completed_at`
  - dataset reference (`dataset_id` or dataset hash), input reference, model config, output reference
  - reproducibility metadata (evaluator version, prompt/template hash)
  - `error` block when status is failed (never silently drop failed runs)

CI replay stance
- Deterministic LLM replay mode in CI is not required for Phases 1-4.
- Reproducibility is enforced via saved datasets, run records, fixed evaluator settings, and schema validation.

Phase 3 config context (V1)
- Treat “config” as:
  - env vars, prompt templates, tool allowlists/tool config, retriever config (when added), model selection
- Store snapshots in-repo under:
  - `configs/snapshots/<tag>/`
- Evidence references in dossiers use:
  - git commit hash + file paths
  - plus a stable `artifact_id` for the diff (see `API.md`)

Agent workflow rules
1) Start with a short plan and the minimal change set.
2) Make small, verifiable increments. After each increment, run the smallest relevant check.
3) Prefer deterministic narrowing before LLM reasoning (hot spans, representative traces).
4) Every output must include evidence pointers: `trace_id`, `span_id`, `artifact_id`.
